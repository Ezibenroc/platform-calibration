#+TITLE: Obtaining Platform Description for SimGrid
#+AUTHOR: Augustin Degomme, Christian Heinrich, Luka Stanisic, ...
#+LANGUAGE:  en
#+STARTUP: inlineimages indent hidestars
#+OPTIONS: H:3 num:t toc:t \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+OPTIONS: TeX:t LaTeX:t skip:nil d:nil todo:nil pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+TAGS: noexport(n)
#+TAGS: LUKA(L) AUGUSTIN(A) CHRISTIAN(C)
#+TAGS: @miriel(m) @stampede(s) @griffon(g) @sirocco(i) @taurus(t)

* Documentation
** README

   Small description of this project, it main goals

** Main steps

  1. Write a platform topology in .xml, consulting platform
     description from hardware vendor or machine administrators. Put
     some default values for bandwidth and latency. In future we hope
     to do this automatically using tools such as netloc.
  2. Calibrate the machine following the steps in "SMPI Calibration's
     documentation" from Augustin Degomme:
     http://simgrid.gforge.inria.fr/contrib/smpi-calibration-doc.php. This
     provides breakpoints and values that should be added to the
     platform description.
  3. To model the backbone, its latency and contention, follow the
     documentation:
     http://simgrid.gforge.inria.fr/contrib/smpi-saturation-doc.php.

* Experiments
** Old initial calibration of homogeneous plafrim miriel machine :LUKA:@miriel:
   
   This is an old example of the workflow, performed even before this
   project was set in place. Paths and code organization have change
   in the new project, but the workflow is similar.

   Measurements were done on plafrim platform, miriel nodes, on 15.06.2016

*** Calibration

   Copy or clone platform_calibration project to the machine you want
   to calibrate.

   There are 2 important files which are used as inputs for the
   calibration: breakpoints and configuration file. They should be
   copied into working folder and edited before running the
   calibration. Pay attention to the names and folder paths. Also
   expected bandwidth and latency chosen for the configuration file
   should be later used for the final platform description.

#+begin_src sh :results output
mkdir analysis/plafrim_15_06_16

rm -rf smpi-calibration/data
mkdir smpi-calibration/data

cp template/testplatform* analysis/plafrim_15_06_16/
#+end_src

   Be sure that macine has /usr/include/libxml2/libxml installed. If
   it is missing, install it and add its path to the Makefile.

#+begin_src sh :results output
#For debian machine simply do
sudo apt-get install libxml2-dev
#+end_src
   
   Compile smpi-calibration on the target machine. If needed add the
   necessary modules.

#+begin_src sh :results output
#For plafrim add for example MPICH module
module load mpi/mpich/ge/gcc/64/3.1

cd smpi-calibration

mpicc -o calibrate calibrate.c experiments.c -I/usr/include/libxml2 -lxml2

cd ..
#+end_src


   When correctly compiled, reserve your nodes (if necessary). Example
   for short reservation of two nodes on plafrim is the following.

#+begin_src sh :results output
salloc -N2 -t 00:30:00
#+end_src

   Now, run the calibration script to measure the machine. Do it
   either by using mpirun/srun (currently not working on plafrim for
   an unknown reason) or by launching the batch script. Here are few
   examples of the command lines to run (choose the one appropriate
   for your machine).

#+begin_src sh :results output
mpirun -np 2 -L miriel026,miriel027 ./calibrate -f data/testplatform.xml
srun -N2 ./calibrate -f data/testplatform.xml
sbatch sbatch_calibration
#+end_src

   Here is an example of the sbatch script for running the
   calibration.

#+begin_src sh :results output
#!/usr/bin/env bash
#Job name
#SBATCH -J calibration
# Asking for N nodes
#SBATCH -N 2
# Output results message
#SBATCH -o %j.out
# Output error message
#SBATCH -e %j.err

# #SBATCH -p longq
module purge
module load slurm/14.03.0
module load mpich/ge/gcc/64/3.1.4

FOLDER=/home-ext/stanisic/platform_calibration

mpirun -np 2 $FOLDER/smpi-calibration/calibrate -f $FOLDER/template/testplatform.xml -s $FOLDER/smpi-calibration/zoo_sizes
#+end_src

#+begin_src sh :results output
sbatch sbatch_calibration
#+end_src

   After executing calibration, on plafrim, delete unnecessary output
   files and finish the reservation. On plafrim for 2 miriel nodes,
   default calibration takes around 1-2 minutes.

#+begin_src sh :results output
rm *.err *.out
scancel -u stanisic
#+end_src

   Copy obtained results in the folder envisioned for the
   analysis. Add analysis file to the same folder as well.

#+begin_src sh :results output
scp plafrim2:platform_calibration/data/* analysis/plafrim_15_06_16/

cp smpi-calibration/Analysis.Rmd analysis/plafrim_15_06_16/
cp smpi-calibration/Makefile analysis/plafrim_15_06_16/
#+end_src

   Now an initial analysis of the obtained data can be performed. If
   you are missing some R packages, consult the initial [[http://simgrid.gforge.inria.fr/contrib/smpi-calibration-doc.php][tutorial]].

#+begin_src sh :results output
cd analysis/plafrim_15_06_16/

make testplatform.html
firefox testplatform.html
#+end_src
   
   Results are promissing, but breakpoints can probably be choosen
   evne better. We can try this configuration.

#+begin_src sh :results output :session org-sh
1420, Small
65536, Medium
131072, Asynchronous
450000, Detached
#+end_src

   Results are not perfect, but there is no need to search for more
   accurate ones as the calibration already probably has significant
   noise and it should be performed much more exhaustively.

   Now the results of the calibrations should be added to the final
   platform description. be sure that the bandwidth and latency values
   written in this final platform description are the same ones use
   for the initial calibration(in testplatform.xml). In the case of
   plafrim, for now we make a simple example.

#+BEGIN_SRC 
<?xml version='1.0'?>
<!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid.dtd">
<!--             _________
                |          |
                |  router  |
    ____________|__________|_____________ backbone
      |   |   |              |     |   |  	
    l0|	l1| l2|           l97| l96 |   | l99
      |   |   |   ........   |     |   |
      |                                |
  node-0.acme.org                  node-99.acme.org
-->
<platform version="3">
	<!--
  <cluster id="acme"
           prefix="node-"    suffix=".acme.org"
           radical="0-99"    power="1Gf" 
           bw="1GBps"      lat="50us"
           bb_bw="10GBps"  bb_lat="500us"/>
--> 

<config id="General">
 <prop id="smpi/os" value="0:1.72337677392447e-06:2.80556540986036e-10;1420:0:0;65536:0:0;131072:0:0;450000:0:0"/>
 <prop id="smpi/ois" value="0:1.79150437795041e-06:2.60957841144294e-10;1420:0:0;65536:0:0;131072:0:0;450000:0:0"/>
 <prop id="smpi/or" value="0:3.27843243070814e-06:3.41675771019744e-09;1420:0:0;65536:0:0;131072:0:0;450000:0:0"/>
 <prop id="smpi/bw-factor" value="0:0.057020586149179;1420:0.955647413834744;65536:1.55105118737739;131072:0.683839496893133;450000:0.613154277278705"/>
 <prop id="smpi/lat-factor" value="0:0.421545218360744;1420:1.15371118224093;65536:1.89572829187563;131072:1.77078433749545;450000:0.698058157888401"/>
 <prop id="smpi/async-small-thres" value="17408"/>
 <prop id="smpi/send-is-detached-thres" value="17408"/>
 <prop id="smpi/wtime" value="4.08585e-08"/>
 <prop id="smpi/iprobe" value="2.56968726731199e-07"/>
 <prop id="smpi/test" value="2.45836195508587e-07"/>
</config>

<AS  id="AS0"  routing="Vivaldi">	 
  <cluster id="acme"
           prefix="node-"    suffix=".acme.org"
           radical="0-99"    power="1Gf" 
	   bw="1GBps"      lat="50us"
           bb_bw="10GBps"  bb_lat="500us"/>
</AS>
   
</platform>
#+END_SRC

     Next phase is to calibrate the backbone link using [[http://simgrid.gforge.inria.fr/contrib/smpi-saturation-doc.php][another tutorial]].

*** Saturation

    The idea is to measure the latency and bandwidth of the backbone
    link, following this [[http://simgrid.gforge.inria.fr/contrib/smpi-saturation-doc.php][tutorial]].
    
    First compile saturation script.

#+begin_src sh :results output
module load mpich/ge/gcc/64/3.1.4

mpicc -o saturation alltoall_loadtest.c
#+end_src

    Then run mpirun/srun or create a batch script like this to run the
    tests.

#+begin_src sh :results output
#!/usr/bin/env bash
#Job name
#SBATCH -J saturation
# Asking for N nodes
#SBATCH -N 4
# Output results message
#SBATCH -o %j.out
# Output error message
#SBATCH -e %j.err

# #SBATCH -p longq
module purge
module load slurm/14.03.0
module load mpich/ge/gcc/64/3.1.4

FOLDER=/home-ext/stanisic/platform_calibration

mpirun -np 4 $FOLDER/smpi-saturation/saturation
#+end_src

#+begin_src sh :results output
sbatch sbatch_saturation
#+end_src

    This took only few seconds for 4 nodes execution.

    Now copy the results on a local machine for the analysis.

#+begin_src sh :results output
mkdir analysis/plafrim_15_06_16/saturation

scp plafrim2:platform_calibration/*.csv analysis/plafrim_15_06_16/saturation

cp smpi-saturation/LoadAnalyze.Rnw analysis/plafrim_15_06_16/saturation
cp smpi-saturation/Makefile analysis/plafrim_15_06_16/saturation
#+end_src
    
    Then delete then data on a target machine.

#+begin_src sh :results output
rm *.csv *.out *.err
#+end_src

    Finally, analyze the measured saturation data.

#+begin_src sh :results output
cd analysis/plafrim_15_06_16/saturation

make
#+end_src

    You might miss french babel package, in which case install it
    with. Also, the initial Makefile is slightly modified when
    cleaning the repository of the .tex files.

#+begin_src sh :results output
sudo aptitude install texlive-lang-french
#+end_src

    Then, inspect the generated .pdf report. At this point it is hard
    to get some some conclusive statements, as only 4 MPI nodes were
    used to test saturation. Probably with 16 or more then results
    will be more interesting.

    The whole workflow seems to work fine, but not sure how to
    interpret results. Probably need to see this with Augustin. In any
    case, if I understood correctly, the latency of the backbone
    should probably be fixed to 0. We will leave the bb_bandwidth at 10GBps for now
    
** New miriel with Omnipath				       :LUKA:@miriel:

   Some part of the miriel cluster should now be connected with new
   Omnipath (100Gb/s) network.

   Calibration was already run on this machine in the past.

*** Calibration

    Cloning the project at the target machine.

#+begin_src sh :results output
git clone git+ssh://stanisic_luka@scm.gforge.inria.fr//gitroot//simgrid/platform-calibration.git

cd platform-calibration.git
#+end_src

    Compiling calibration scripts.

#+begin_src sh :results output
module load mpi/mpich/ge/gcc/64/3.1

cd src/calibration

mpicc -o calibrate calibrate.c experiments.c -I/usr/include/libxml2 -lxml2

cd -
#+end_src
   
    Creating and preparing new platform folder. Adapting
    testplatform.xml for a new folder path and bandwidth+latency.

#+begin_src sh :results output
mkdir data/miriel_27_06_2016

cp template/testplatform* data/miriel_27_06_2016
#+end_src

    

    Allocating and running calibration scripts.

#+begin_src sh :results output
salloc -N2 -t 00:30:00

sbatch sbatch_plafrim_calibration
#+end_src

   Here is an example of the sbatch script for running the
   calibration.

#+begin_src sh :results output
#!/usr/bin/env bash
#Job name
#SBATCH -J calibration
# Asking for N nodes
#SBATCH -N 2
# Output results message
#SBATCH -o %j.out
# Output error message
#SBATCH -e %j.err

# #SBATCH -p longq
module purge
module load slurm/14.03.0
module load mpich/ge/gcc/64/3.1.4

FOLDER=/home-ext/stanisic/platform-calibration
DATAFOLDER="miriel_27_06_2016"

mpirun -np 2 $FOLDER/src/calibration/calibrate -f $FOLDER/data/$DATAFOLDER/testplatform.xml -s $FOLDER/src/calibration/zoo_sizes
#+end_src

   Data is saved in data/miriel_27_06_2016.

   After executing calibration, on plafrim, delete unnecessary output
   files and finish the reservation. On plafrim for 2 miriel nodes,
   default calibration takes around 1-2 minutes.

#+begin_src sh :results output
rm *.err *.out
scancel -u stanisic
#+end_src

   Now on a local machine pull new data and run the analysis (all R
   packages and similar have already been installed before).

#+begin_src sh :results output
cd data/miriel_27_06_2016/

cp ../../src/calibration/Makefile ../../src/calibration/Analysis.Rmd  

make testplatform.html
firefox testplatform.html
#+end_src

   The data is a bit noisy, but the breakpoints can be
   improved. Change testplatform_breakpoints to these values.

#+begin_src sh :results output
Limit, Name
1420, Small
65536, Medium
131072, Asynchronous
400000, Detached
#+end_src

   Now results are quite good. Actually, it is probably wise to
   regroup everything in a separate folder "calibration", inside the
   miriel_27_06_2016 data folder.

   The final output of the analysis is the following.

#+BEGIN_SRC 
## <config id="General">
##  <prop id="smpi/os" value="0:1.70323849188712e-06:2.79726695925964e-10;1420:0:0;65536:0:0;131072:0:0;4e+05:0:0"/>
##  <prop id="smpi/ois" value="0:1.75232188013067e-06:2.20494312485917e-10;1420:0:0;65536:0:0;131072:0:0;4e+05:0:0"/>
##  <prop id="smpi/or" value="0:3.33315583120708e-06:3.40864964492857e-09;1420:0:0;65536:0:0;131072:0:0;4e+05:0:0"/>
##  <prop id="smpi/bw-factor" value="0:0.0636575813349588;1420:0.983495291166692;65536:1.52596144919948;131072:0.717251283995903;4e+05:0.619663249676955"/>
##  <prop id="smpi/lat-factor" value="0:0.424821296069165;1420:1.16678508115501;65536:1.86811479079315;131072:2.0798208704917;4e+05:0.923798288770192"/>
##  <prop id="smpi/async-small-thres" value="17408"/>
##  <prop id="smpi/send-is-detached-thres" value="17408"/>
##  <prop id="smpi/wtime" value="4.085767e-08"/>
##  <prop id="smpi/iprobe" value="2.75181277860327e-07"/>
##  <prop id="smpi/test" value="2.60107333333333e-07"/>
## </config>
#+END_SRC


*** Saturation

    Compiling code on the target machine.

#+begin_src sh :results output
cd src/saturation

module load mpich/ge/gcc/64/3.1.4

mpicc -o saturation alltoall_loadtest.c

cd -
#+end_src

    Launching saturation script. Normally the script itself doesnt
    take so much time to execute, but since it demands for 16 nodes,
    this can take some time.

#+begin_src sh :results output
sbatch sbatch_plafrim_saturation
#+end_src

    The example of the batch script.

#+begin_src sh :results output
#!/usr/bin/env bash
#Job name
#SBATCH -J saturation
# Asking for N nodes
#SBATCH -N 16
# Output results message
#SBATCH -o %j.out
# Output error message
#SBATCH -e %j.err

#SBATCH -p longq
module purge
module load slurm/14.03.0
module load mpich/ge/gcc/64/3.1.4

FOLDER=/home-ext/stanisic/platform-calibration

mpirun -np 16 $FOLDER/src/saturation/saturation
#+end_src

    When the execution is finished, copy the results into the right
    data folder.

#+begin_src sh :results output
mkdir data/miriel_27_06_2016/saturation

cp *.csv data/miriel_27_06_2016/saturation
#+end_src

    Delete the unnecessary files.

#+begin_src sh :results output
rm *.err *.out *.csv
scancel -u stanisic
#+end_src

    Finally, analyze the measured saturation data on the local
    machine. Check the generated .pdf, especially looking at the
    second figure, which should provide the "bb_ban" value.

#+begin_src sh :results output
cd data/miriel_27_06_2016/saturation

cp ../../../src/saturation/LoadAnalyze.Rnw ../../../src/saturation/Makefile .

make
#+end_src

    Everything went fine, but the values tested are not well adapted
    for the Omnipath network, as we have not yet reach the peak of the
    link bandwidth.

    Probably need to redo this saturation tests with different input
    values, to better test limits of the network.

** Taurus machine in Lyon                                :CHRISTIAN:@taurus:
*** Reservation

The taurus machine shares a single switch with both orion and hercule
clusters. We’ve observed that experiments are not reliable if these
two clusters are not reserved as well, since the switch does not
isolate all clusters from each other. 

The command that I used to reserve the clusters:

#+begin_src sh :results output :exports both
oarsub -t deploy -l "{cluster in ('taurus', 'orion', 'hercule')}/nodes=ALL,walltime=00:55" -r "$(date '+%Y-%m-%d %H:%M:%S')"
#+end_src

This attempts to obtain a reservation right away, but one can modify the
reservation for a specific date like this:

#+begin_src sh :results output :exports both
oarsub -t deploy -l "{cluster in ('taurus', 'orion', 'hercule')}/nodes=ALL,walltime=12:50" -r "$(date '+%Y-%m-%d %H:%M:%S' --date="2016-07-25 19:00:00")"
#+end_src

Anyways, after that I deployed my own =debian_stretch= image via
=kadeploy3=:

#+begin_src sh :results output :exports both
kadeploy3 -a /home/cheinrich/debian_stretch.env -f $OAR_FILE_NODES -k
#+end_src

*** Calibration

I already had a taurus.xml set up and since I wanted to run the
calibration several times, I just copied it to all the
directories. The actual taurus.xml can now be found at [[file:./data/taurus/calibration/taurus.xml]].

#+begin_src sh :results output :exports both
cd src/smpi-calibration
for i in {1..9}; do
  mkdir taurus-2016-06-09–$i
  cp taurus/taurus.xml taurus-2016-06-09--$i
  cp taurus/taurus_breakpoints taurus-2016-06-09--$i
  sed -i -e 's/"taurus"/"taurus-2016-06-09--'$i'"/' taurus-2016-06-09--$i/taurus.xml
done
#+end_src

Then I compiled the calibration:

#+begin_src sh :results output :exports both
mpicc -o calibrate calibrate.c experiments.c -I/usr/include/libxml2 -lxml2
#+end_src

and executed it (after =taurus-hostnames= was created and/or updated):

#+begin_src sh :results output :exports both
for i in {1..9}; do
  mpirun --report-bindings -bycore -bind-to-core --timestamp-output -np 2 --mca pml ob1 --mca btl tcp,self -machinefile ./machinefile ./calibrate -f taurus-2016-06-09--$i/taurus.xml
  sleep 20
done
#+end_src

I added the =sleep= to allow some time to pass (just in case there was something going on in the system).

*** Saturation

#+begin_src sh :results output :exports both
uniq $OAR_NODE_FILE | /bin/grep "taurus" >| taurus-hostnames
mpicc alltoall_loadtest.c -o saturation -O3
mpirun -np 12 --mca plm_rsh_agent 'ssh' --mca pml ob1 --mca btl tcp,self -machinefile ./taurus-hostnames saturation
#+end_src
** Sirocco machine in Bordeaux                               :LUKA:@sirocco:

This is a small GPU cluster, which is a part of plafrim, situated in
Bordeaux. There are only 5 nodes, but each one has 2 Dodeca-core
Haswell Intel® Xeon® E5-2680 CPUs and 4 Nvidia K40 GPUs.

More precise architecture description is available [[https://plafrim.bordeaux.inria.fr/doku.php?id=plateforme:configurations:sirocco][here]].

This is a machine we want StarPU-MPI + SimGrid to work on.

Just like for the miriel, I was not sure what default latency to put,
so it was 10 microseconds.

Also how to know whether Ethernet or IB is used? And what will StarPU
use and how? Which MPI implementation shall I use? Need to discuss
these things with Samuel.

For now doing a basic calibration and saturation, just to be sure we have some kind of platform description. Later it will be improved.

*** Calibration

Compiling calibration scripts.

#+begin_src sh :results output
module load mpi/mpich/ge/gcc/64/3.1

cd src/calibration

mpicc -o calibrate calibrate.c experiments.c -I/usr/include/libxml2 -lxml2

cd -
#+end_src

Creating and preparing new platform folder. Adapting testplatform.xml
for a new folder path and bandwidth+latency.

#+begin_src sh :results output
mkdir data/sirocco_01_08_2016
mkdir data/sirocco_01_08_2016/calibration

cp template/testplatform* data/sirocco_01_08_2016/calibration
#+end_src


Allocating and running calibration scripts (slightly different for sirocco than for miriel).

#+begin_src sh :results output
salloc -N2 --exclusive -t 00:30:00 -p court_sirocco -x sirocco06

sbatch data/sirocco_01_08_2016/sbatch_plafrim_calibration
#+end_src

   Here is an example of the sbatch script for running sirocco
   calibration.

#+begin_src sh :results output
#!/usr/bin/env bash
#Job name
#SBATCH -J calibration
# Asking for N nodes
#SBATCH -N 2
# Output results message
#SBATCH -o %j.out
# Output error message
#SBATCH -e %j.err

# #SBATCH -p longq
# Asking for sirocco nodes
#SBATCH -p court_sirocco -x sirocco06

module purge
module load slurm/14.03.0
module load mpich/ge/gcc/64/3.1.4

FOLDER="/home-ext/stanisic/platform-calibration"
DATAFOLDER="data/sirocco_01_08_2016/calibration"

mpirun -np 2 $FOLDER/src/calibration/calibrate -f $FOLDER/$DATAFOLDER/testplatform.xml -s $FOLDER/src/calibration/zoo_sizes
#+end_src


Data is saved in data/sirocco_01_08_2016/calibration and pushed to the
remote repository.

#+begin_src shell :results output
git add data/sirocco_01_08_2016
git commit -m "Adding calibration data on sirocco"
git push
#+end_src

After executing calibration, on plafrim, delete unnecessary output
files and finish the reservation. On plafrim for 2 sirocco nodes,
default calibration takes around 1-2 minutes.

#+begin_src sh :results output
rm *.err *.out
scancel -u stanisic
#+end_src

 Now on a local machine pull new data and run the analysis (all R
 packages and similar have already been installed before).

#+begin_src sh :results output
cd data/sirocco_01_08_2016/calibration

cp ../../../src/calibration/Makefile ../../../src/calibration/Analysis.Rmd . 

make testplatform.html
firefox testplatform.html
#+end_src

The data is a bit noisy, but the breakpoints can be improved. Change
testplatform_breakpoints to these values (the same ones used for
miriel).

#+begin_src sh :results output
Limit, Name
1420, Small
65536, Medium
131072, Asynchronous
400000, Detached
#+end_src

Now results are quite good, the final output of the analysis is the
following.

#+BEGIN_SRC 
## <config id="General">
##  <prop id="smpi/os" value="0:2.70683864220123e-06:2.47269722619783e-10;1420:3.97545175056929e-06:7.23786118047894e-11;65536:7.58804087458976e-06:6.77088730090102e-11;131072:0:0;4e+05:0:0"/>
##  <prop id="smpi/ois" value="0:2.7741781272553e-06:2.1631688187721e-10;1420:4.03664653739977e-06:6.92199520863807e-11;65536:5.77600529533672e-06:8.19394370464538e-11;131072:2.46349531812281e-06:1.48757168395357e-13;4e+05:2.43434499482972e-06:3.03855696558919e-13"/>
##  <prop id="smpi/or" value="0:5.24254663627748e-06:2.05940556065388e-09;1420:1.13604897462265e-05:2.8332717225738e-10;65536:7.36920463371884e-06:3.32186380684382e-10;131072:0:0;4e+05:0:0"/>
##  <prop id="smpi/bw-factor" value="0:0.124230489840844;1420:0.311345781078732;65536:0.344336843153613;131072:0.290935589385778;4e+05:0.291581596908999"/>
##  <prop id="smpi/lat-factor" value="0:0.737322772282333;1420:1.16891951785859;65536:1.55145779857452;131072:4.44599650869728;4e+05:4.47543800174862"/>
##  <prop id="smpi/async-small-thres" value="65536"/>
##  <prop id="smpi/send-is-detached-thres" value="320000"/>
##  <prop id="smpi/wtime" value="3.098699e-08"/>
##  <prop id="smpi/iprobe" value="2.33927297668038e-07"/>
##  <prop id="smpi/test" value="2.28018867924528e-07"/>
## </config>
#+END_SRC

*** Saturation

Probably no need to do spend time on this at this point, as first the calibration needs to be improved.
** Advanced sirocco machine in Bordeaux                      :LUKA:@sirocco:

This time we have a bit more information on the machine (latency 1us,
real bandwidth 4GB/s), so we want to redo properly another calibration
and saturation. Also we want to use OpenMPI not MPICH this time.

Also we want to test the transfer time for a specific 8MB data
transfer, as this is the only one used when doing tile Cholesky
factorization from Chameleon on sirocco nodes, since the tile size
is 1440.

*** Calibration

Compiling calibration scripts on sirocco machine.

#+begin_src sh :results output
module load mpi/openmpi

cd src/calibration

mpicc -o calibrate calibrate.c experiments.c -I/usr/include/libxml2 -lxml2

cd -
#+end_src

Creating and preparing new platform folder. Adapting testplatform.xml
for a new folder path and bandwidth+latency.

#+begin_src sh :results output
mkdir data/sirocco_26_08_2016
mkdir data/sirocco_26_08_2016/calibration

cp template/testplatform* data/sirocco_26_08_2016/calibration
cp sbatch_plafrim_calibration  data/sirocco_26_08_2016
#+end_src


Allocating and running calibration scripts (slightly different for
sirocco than for miriel).  Need to adapt the
data/sirocco_26_08_2016/calibration/testplatform.xml file for the
output directory, platform name, expected latency and expected bandwidth (not sure if
it is in GB/s or in Gb/s).

#+begin_src sh :results output
sbatch data/sirocco_26_08_2016/sbatch_plafrim_calibration
#+end_src

Data is saved in data/sirocco_26_08_2016/calibration and pushed to the
remote repository.

#+begin_src shell :results output
git add data/sirocco_26_08_2016
git commit -m "Adding calibration data on sirocco"
git push
#+end_src

After executing calibration, on plafrim, delete unnecessary output
files and finish the reservation. On plafrim for 2 sirocco nodes,
default calibration takes around 1-2 minutes.

#+begin_src sh :results output
rm *.err *.out
scancel -u stanisic
#+end_src

 Now on a local machine pull new data and run the analysis (all R
 packages and similar have already been installed before).

#+begin_src sh :results output
cd data/sirocco_26_08_2016/calibration

cp ../../../src/calibration/Makefile ../../../src/calibration/Analysis.Rmd . 

make testplatform.html
firefox testplatform.html
#+end_src

The results are quite stable and here is the output.

#+BEGIN_SRC 
## <config id="General">
##  <prop id="smpi/os" value="0:2.26208782197973e-07:8.3096367905883e-11;1420:9.08119677184139e-08:1.5705037478521e-10;7000:2.35718813151922e-07:1.25874360533578e-10;174080:-1.25337841118449e-05:1.94161826581092e-10;260000:0:0"/>
##  <prop id="smpi/ois" value="0:2.4225468335082e-07:8.78040749071613e-11;1420:2.13616351795046e-07:1.44607110369053e-10;7000:3.17303568995753e-07:1.22737206281853e-10;174080:1.4795323742668e-05:3.85796062588106e-11;260000:4.2676375347139e-07:2.76393692773138e-13"/>
##  <prop id="smpi/or" value="0:8.00841235810377e-07:3.5577540908684e-10;1420:5.1045941579316e-07:4.49845689659618e-10;7000:1.55437952808834e-06:3.63232330966495e-10;174080:9.87962740697632e-06:3.20352025352883e-10;260000:0:0"/>
##  <prop id="smpi/bw-factor" value="0:0.164159935554902;1420:0.529279816247861;7000:1.15239187605285;174080:1.2410545495175;260000:0.678376089946641"/>
##  <prop id="smpi/lat-factor" value="0:1.44747969855069;1420:3.21984228299524;7000:6.14693915739059;174080:5.05233151986717;260000:15.6349316380971"/>
##  <prop id="smpi/async-small-thres" value="65536"/>
##  <prop id="smpi/send-is-detached-thres" value="320000"/>
##  <prop id="smpi/wtime" value="4.411823e-08"/>
##  <prop id="smpi/iprobe" value="2.01049751243781e-07"/>
##  <prop id="smpi/test" value="6.82e-07"/>
## </config>
#+END_SRC

*** Transfer of 1440 single precision tile (~8MB of data)

On the plafrim create folder and prepare input (also adapt xml file
and sbatch). Also do not forget to adapt minSize and maxSize values in
the .xml file.

#+begin_src shell :results output
mkdir data/sirocco_26_08_2016/8MB_transfer
cp template/testplatform* data/sirocco_26_08_2016/8MB_transfer
cp sbatch_plafrim_calibration data/sirocco_26_08_2016/sbatch_plafrim_calibration2

for i in {1..435}; do echo $((1440*1440*4)) >> src/calibration/8MB_size; done
#+end_src

Now launching the execution.

#+begin_src sh :results output
sbatch data/sirocco_26_08_2016/sbatch_plafrim_calibration
#+end_src

Data is saved in data/sirocco_26_08_2016/calibration and pushed to the
remote repository.

#+begin_src shell :results output
git add data/sirocco_26_08_2016
git commit -m "Adding calibration data on sirocco for 8MB transfer"
git push
#+end_src

After executing calibration, on plafrim, delete unnecessary output
files and finish the reservation. On plafrim for 2 sirocco nodes,
default calibration takes around 1-2 minutes.

#+begin_src sh :results output
rm *.err *.out
scancel -u stanisic
#+end_src

 Now on a local machine pull new data and run the analysis (all R
 packages and similar have already been installed before). Also need
 to adapt breakpoints to use this single value

#+begin_src sh :results output
cd data/sirocco_26_08_2016/8MB_transfer

cp ../../../src/calibration/Makefile ../../../src/calibration/Analysis.Rmd . 

make testplatform.html
firefox testplatform.html
#+end_src

Standard analysis is not adapted for this kind of automatic analysis,
so better to do it by hand.

*** Analysis 8MB transfers
Let's analyze duration of MPI_Send, MPI_Isend and MPI_Recv one by one.

#+begin_src R :results output graphics :file :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session org-R :dir .
library(ggplot2)

read_csv <- function(file) {
  df<-read.csv(file,header=FALSE)
  names(df)<-c("Operation", "Size", "Start", "Duration")

  df
}
df<-rbind(read_csv("data/sirocco_26_08_2016/8MB_transfer/sirocco_Isend.csv"),
          #read_csv("data/sirocco_26_08_2016/8MB_transfer/sirocco_Isend.csv"),
          read_csv("data/sirocco_26_08_2016/8MB_transfer/sirocco_Recv.csv"))


ggplot(df, aes(Duration))+geom_histogram()+xlab("Duration [s]")+facet_wrap(~Operation, ncol=1, scales="free")
#+end_src

#+RESULTS:
[[file:/tmp/babel-3188UQA/figure3188MnL.png]]

  OK these are not too long. Let's see for the whole transfer.

#+begin_src R :results output graphics :file :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session org-R :dir .
library(ggplot2)

read_csv <- function(file) {
  df<-read.csv(file,header=FALSE)
  names(df)<-c("Operation", "Size", "Start", "Duration")

  df
}
df<-read_csv("data/sirocco_26_08_2016/8MB_transfer/sirocco_PingPong.csv")

df_send<-df[df$Operation=="MPI_Send",]
df_recvs<-df[df$Operation=="MPI_Recv",]


df2 <- data.frame( Operation = "Transfer",
                   Size = df_send$Size,
                   Start = df_send$Start,
                   Duration = df_recvs$Duration + df_send$Duration)

df<-rbind(df,df2)

ggplot(df, aes(Duration))+geom_histogram()+xlab("Duration [s]")+facet_wrap(~Operation, ncol=1)
#+end_src

#+RESULTS:
[[file:/tmp/babel-3188UQA/figure3188m7X.png]]


  Actually these are very stable!
